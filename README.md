# End-to-End-RAG-Pipeline-with-Multi-LLM-Benchmarking
# End-to-End RAG Pipeline with Multi-LLM Benchmarking

> A complete **Retrieval-Augmented Generation (RAG)** system for English
> extractive Question Answering â€” benchmarking **LLaMA-3.8B, Qwen2.5-7B &
> Mistral-7B** with FAISS dense retrieval vs. No-RAG baselines across
> 500 queries.

---

## Overview

This project processes **50 English documents** into **449 overlapping chunks**,
builds a **FAISS vector index** using `sentence-transformers/all-MiniLM-L6-v2`
embeddings, and generates short extractive answers (â‰¤5 words) for **500 queries**
using locally hosted LLMs via **Ollama**. Both RAG and No-RAG modes are rigorously
evaluated using exact match, substring match, and semantic similarity metrics.

---

## Results

### Answerable Queries (350 questions)

| Metric | LLaMA-3.8B | Qwen2.5-7B | Mistral-7B |
|---|---|---|---|
| Exact Match â€” RAG | 0.540 | **0.609** | 0.506 |
| Exact Match â€” No-RAG | 0.089 | 0.211 | 0.151 |
| Substring Match â€” RAG | 0.771 | **0.774** | 0.700 |
| Semantic Match â‰¥0.75 â€” RAG | 0.731 | **0.786** | 0.720 |
| Semantic Mean Score â€” RAG | 0.815 | **0.850** | 0.823 |

### Unanswerable Queries â€” NA Detection (150 questions)

| Model | RAG | No-RAG |
|---|---|---|
| LLaMA-3.8B | 0.820 | 0.553 |
| Qwen2.5-7B | **0.873** | **0.940** |
| Mistral-7B | 0.080 | 0.000 |

> âœ… **Qwen2.5-7B + RAG** is the top-performing model across all metrics.

---

## ðŸ›  Tech Stack

| Component | Tool |
|---|---|
| Embeddings | `sentence-transformers/all-MiniLM-L6-v2` (384-dim) |
| Vector Store | FAISS `IndexFlatIP` (cosine similarity) |
| LLMs | LLaMA-3.8B Â· Qwen2.5-7B Â· Mistral-7B via Ollama |
| Language | Python 3.11 |
| Libraries | Pandas Â· NumPy Â· scikit-learn Â· tqdm Â· transformers |

---

## Pipeline Architecture

```
Raw Documents (50 English docs)
        â”‚
        â–¼
  Unicode Cleaning + Evidence Validation
        â”‚
        â–¼
  Sliding Window Chunking (150w, 30w overlap)
        â”‚  â†’ 449 total chunks
        â–¼
  MiniLM Embeddings â†’ FAISS Index
        â”‚
        â–¼
  Query â†’ Top-3 Chunk Retrieval (cosine sim)
        â”‚
        â–¼
  Strict RAG Prompt â†’ Ollama LLM â†’ â‰¤5 word answer / NA
        â”‚
        â–¼
  Evaluation: Exact / Substring / Semantic Match
        â”‚
        â–¼
  EVALUATION_ALL_MODELS.csv
```

---

## Project Structure

```
â”œâ”€â”€ phase1_english_pipeline.ipynb    # Main pipeline notebook
â”œâ”€â”€ E2/
â”‚   â”œâ”€â”€ documents.csv                # 50 source documents
â”‚   â”œâ”€â”€ queries.csv                  # 500 questions
â”‚   â”œâ”€â”€ answers.csv                  # Gold answers + evidence spans
â”‚   â”œâ”€â”€ results.csv                  # LLaMA RAG results
â”‚   â”œâ”€â”€ resultsfull.csv              # All model outputs
â”‚   â””â”€â”€ EVALUATION_ALL_MODELS.csv    # Master evaluation table
```

---

## Quickstart

```bash
# 1. Install dependencies
pip install sentence-transformers faiss-cpu pandas numpy scikit-learn tqdm transformers

# 2. Pull Ollama models
ollama pull llama3:8b
ollama pull qwen2.5:7b
ollama pull mistral:7b

# 3. Run the notebook
jupyter notebook phase1_english_pipeline.ipynb
```

---

## ðŸ“ˆ Key Findings

- RAG improves exact match by **3â€“6Ã—** over No-RAG across all three models
- **Qwen2.5-7B** is the best model: 60.9% exact match + 87.3% correct NA detection
- **Mistral-7B** nearly fails on unanswerable queries (only 8% NA detection with RAG)
- All RAG models reach semantic mean scores of **0.82â€“0.85**, confirming
  high answer quality even when exact match falls short

---

## ðŸ“„ License

MIT
