{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723ba11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kisho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf23ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Project2\\data\\en\\documents.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = r\"E:\\Project2\\data\\en\"\n",
    "\n",
    "documents_path = os.path.join(BASE_PATH, \"documents.csv\")\n",
    "queries_path = os.path.join(BASE_PATH, \"queries.csv\")\n",
    "answers_path = os.path.join(BASE_PATH, \"answers.csv\")\n",
    "\n",
    "print(documents_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97624cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: (50, 2)\n",
      "Queries: (500, 4)\n",
      "Answers: (500, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>Albert Einstein was a German-born theoretical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>Asia is the largest continent in the world by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>Bengaluru also known as Bangalore is the capit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>Biology is the scientific study of life and li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>Buddhism also known as Buddha-dharma and Dharm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                      document_text\n",
       "0  doc_1  Albert Einstein was a German-born theoretical ...\n",
       "1  doc_2  Asia is the largest continent in the world by ...\n",
       "2  doc_3  Bengaluru also known as Bangalore is the capit...\n",
       "3  doc_4  Biology is the scientific study of life and li...\n",
       "4  doc_5  Buddhism also known as Buddha-dharma and Dharm..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df = pd.read_csv(documents_path)\n",
    "queries_df = pd.read_csv(queries_path)\n",
    "answers_df = pd.read_csv(answers_path)\n",
    "\n",
    "print(\"Documents:\", documents_df.shape)\n",
    "print(\"Queries:\", queries_df.shape)\n",
    "print(\"Answers:\", answers_df.shape)\n",
    "\n",
    "documents_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize to NFC\n",
    "    text = unicodedata.normalize(\"NFC\", str(text))\n",
    "    \n",
    "    # Remove zero-width characters\n",
    "    text = re.sub(r\"[\\u200B\\u200C\\u200D\\uFEFF]\", \"\", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39569d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df[\"document_text\"] = documents_df[\"document_text\"].apply(clean_unicode)\n",
    "queries_df[\"question\"] = queries_df[\"question\"].apply(clean_unicode)\n",
    "answers_df[\"answer\"] = answers_df[\"answer\"].apply(clean_unicode)\n",
    "answers_df[\"evidence_span\"] = answers_df[\"evidence_span\"].apply(clean_unicode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489a3185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers >5 words:\n",
      "         query_id  doc_id                                             answer  \\\n",
      "180  doc_19_q0181  doc_19                      get the ball over a goal line   \n",
      "215  doc_22_q0216  doc_22     Sankhya Yoga Nyaya Vaisheshika Mimamsa Vedanta   \n",
      "245  doc_25_q0246  doc_25  Internet Corporation for Assigned Names and Nu...   \n",
      "371  doc_38_q0372  doc_38             Matter, energy, force, space, and time   \n",
      "456  doc_46_q0457  doc_46  What percentage of households owned a TV in 2013?   \n",
      "\n",
      "                                         evidence_span  word_count  \n",
      "180  the object is to get the ball over a goal line...           7  \n",
      "215  The six stika schools of Hindu philosophy that...           6  \n",
      "245  the Domain Name System are directed by a maint...           7  \n",
      "371  hysics is the scientific study of matter its f...           6  \n",
      "456  In 2013 79% of the world's households owned a ...           9  \n",
      "Invalid evidence spans:\n",
      "         query_id  doc_id                         answer  \\\n",
      "13    doc_2_q0014   doc_2                   Indian Ocean   \n",
      "23    doc_3_q0024   doc_3                    Garden City   \n",
      "25    doc_3_q0026   doc_3                  Kempe Gowda I   \n",
      "35    doc_4_q0036   doc_4           cell genes evolution   \n",
      "40    doc_5_q0041   doc_5                        Lumbini   \n",
      "43    doc_5_q0044   doc_5    blowing out of the passions   \n",
      "44    doc_5_q0045   doc_5         Theravada and Mahayana   \n",
      "50    doc_6_q0051   doc_6             carnivorous mammal   \n",
      "62    doc_7_q0063   doc_7                        alchemy   \n",
      "136  doc_14_q0137  doc_14                  majority rule   \n",
      "166  doc_17_q0167  doc_17                          Latin   \n",
      "192  doc_20_q0193  doc_20                          Paris   \n",
      "195  doc_20_q0196  doc_20                             18   \n",
      "206  doc_21_q0207  doc_21                           1871   \n",
      "246  doc_25_q0247  doc_25         Vint Cerf and Bob Kahn   \n",
      "276  doc_28_q0277  doc_28                         shogun   \n",
      "286  doc_29_q0287  doc_29             Tryst with Destiny   \n",
      "304  doc_31_q0305  doc_31                    City of Joy   \n",
      "323  doc_33_q0324  doc_33                  Bhagavad Gita   \n",
      "326  doc_33_q0327  doc_33                   24000 verses   \n",
      "353  doc_36_q0354  doc_36                      29.5 days   \n",
      "372  doc_38_q0373  doc_38                      physicist   \n",
      "380  doc_39_q0381  doc_39               Bengali polymath   \n",
      "383  doc_39_q0384  doc_39                      Gitanjali   \n",
      "391  doc_40_q0392  doc_40          Electromagnetic waves   \n",
      "430  doc_44_q0431  doc_44                           1969   \n",
      "440  doc_45_q0441  doc_45      telecommunications device   \n",
      "441  doc_45_q0442  doc_45  sound into electronic signals   \n",
      "486  doc_49_q0487  doc_49              universal solvent   \n",
      "\n",
      "                                         evidence_span  word_count  \\\n",
      "13            bounded on the south by the Indian Ocean           2   \n",
      "23            The city is known as India's Garden City           2   \n",
      "25     In 1537 CE Kempe Gowda I established a mud fort           3   \n",
      "35   five fundamental themes: the cell genes and he...           3   \n",
      "40           He was born in Lumbini, present-day Nepal           1   \n",
      "43   attaining nirvana the blowing out of the passions           5   \n",
      "44   Two major extant branches of Buddhism are gene...           3   \n",
      "50               The cat is a small carnivorous mammal           2   \n",
      "62   The word chemistry comes from a modification o...           1   \n",
      "136                 Majority rule is the dominant form           2   \n",
      "166  The term education is derived from the Latin w...           1   \n",
      "192                  Its capital largest city is Paris           1   \n",
      "195  Its 18 integral regions five of which are over...           1   \n",
      "206  Unification of Germany established the German ...           1   \n",
      "246  Vint Cerf at Stanford University and Bob Kahn ...           5   \n",
      "276  From the 12th century actual power was held by...           1   \n",
      "286  Nehru gave a critically acclaimed speech Tryst...           3   \n",
      "304           Kolkata is also known as the City of Joy           3   \n",
      "323  Among the principal works in the Mahbhrata are...           2   \n",
      "326  the Mahabharata itself distinguishes a core po...           2   \n",
      "353              It completes an orbit every 29.5 days           2   \n",
      "372  A scientist who specializes in physics is call...           1   \n",
      "380         Rabindranath Thakur was a Bengali polymath           2   \n",
      "383       He was the author of the poetry of Gitanjali           1   \n",
      "391  Radio waves are electromagnetic waves of frequ...           2   \n",
      "430        The state was renamed as Tamil Nadu in 1969           1   \n",
      "440  A telephone is a telecommunications device tha...           2   \n",
      "441  A telephone converts sound into electronic sig...           4   \n",
      "486  water is often referred to as the universal so...           2   \n",
      "\n",
      "                                         document_text  evidence_valid  \n",
      "13   Asia is the largest continent in the world by ...           False  \n",
      "23   Bengaluru also known as Bangalore is the capit...           False  \n",
      "25   Bengaluru also known as Bangalore is the capit...           False  \n",
      "35   Biology is the scientific study of life and li...           False  \n",
      "40   Buddhism also known as Buddha-dharma and Dharm...           False  \n",
      "43   Buddhism also known as Buddha-dharma and Dharm...           False  \n",
      "44   Buddhism also known as Buddha-dharma and Dharm...           False  \n",
      "50   The cat also called domestic cat and house cat...           False  \n",
      "62   Chemistry is the scientific study of the prope...           False  \n",
      "136  Democracy is a form of government in which pol...           False  \n",
      "166  Education is the transmission of knowledge and...           False  \n",
      "192  France officially the French Republic is a cou...           False  \n",
      "195  France officially the French Republic is a cou...           False  \n",
      "206  Germany officially the Federal Republic of Ger...           False  \n",
      "246  The Internet is the global system of interconn...           False  \n",
      "276  Japan is an island country in East Asia. Locat...           False  \n",
      "286  Jawaharlal Nehru was an Indian anti-colonial n...           False  \n",
      "304  Kolkata formerly known as Calcutta is the capi...           False  \n",
      "323  The Mahbhrata is a smriti text from ancient In...           False  \n",
      "326  The Mahbhrata is a smriti text from ancient In...           False  \n",
      "353  The Moon is the only natural satellite of Eart...           False  \n",
      "372  Physics is the scientific study of matter its ...           False  \n",
      "380  Rabindranath Thakur also known by his pseudony...           False  \n",
      "383  Rabindranath Thakur also known by his pseudony...           False  \n",
      "391  Radio is the technology of communicating using...           False  \n",
      "430  Tamil Nadu is the southernmost state of India....           False  \n",
      "440  A telephone commonly shortened to phone is a t...           False  \n",
      "441  A telephone commonly shortened to phone is a t...           False  \n",
      "486  Water is an inorganic compound with the chemic...           False  \n"
     ]
    }
   ],
   "source": [
    "# 1. Check ≤5 word answers\n",
    "answers_df[\"word_count\"] = answers_df[\"answer\"].apply(lambda x: len(x.split()) if x != \"NA\" else 0)\n",
    "print(\"Answers >5 words:\")\n",
    "print(answers_df[answers_df[\"word_count\"] > 5])\n",
    "\n",
    "# 2. Evidence span exists in document\n",
    "merged = answers_df.merge(documents_df, on=\"doc_id\")\n",
    "\n",
    "def evidence_check(row):\n",
    "    if row[\"answer\"] == \"NA\":\n",
    "        return True\n",
    "    return row[\"evidence_span\"] in row[\"document_text\"]\n",
    "\n",
    "merged[\"evidence_valid\"] = merged.apply(evidence_check, axis=1)\n",
    "print(\"Invalid evidence spans:\")\n",
    "print(merged[merged[\"evidence_valid\"] == False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505584c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>evidence_span</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>doc_46_q0457</td>\n",
       "      <td>doc_46</td>\n",
       "      <td>What percentage of households owned a TV in 2013?</td>\n",
       "      <td>In 2013 79% of the world's households owned a ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         query_id  doc_id                                             answer  \\\n",
       "456  doc_46_q0457  doc_46  What percentage of households owned a TV in 2013?   \n",
       "\n",
       "                                         evidence_span  word_count  \n",
       "456  In 2013 79% of the world's households owned a ...           9  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate the row first (sanity check)\n",
    "answers_df[answers_df[\"query_id\"] == \"doc_46_q0457\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b8da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.loc[\n",
    "    answers_df[\"query_id\"] == \"doc_46_q0457\",\n",
    "    \"answer\"\n",
    "] = \"79%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da2ae28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>evidence_span</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>doc_46_q0457</td>\n",
       "      <td>doc_46</td>\n",
       "      <td>79%</td>\n",
       "      <td>In 2013 79% of the world's households owned a ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         query_id  doc_id answer  \\\n",
       "456  doc_46_q0457  doc_46    79%   \n",
       "\n",
       "                                         evidence_span  word_count  \n",
       "456  In 2013 79% of the world's households owned a ...           9  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_df[answers_df[\"query_id\"] == \"doc_46_q0457\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b753bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers >5 words:\n",
      "         query_id  doc_id                                             answer  \\\n",
      "180  doc_19_q0181  doc_19                      get the ball over a goal line   \n",
      "215  doc_22_q0216  doc_22     Sankhya Yoga Nyaya Vaisheshika Mimamsa Vedanta   \n",
      "245  doc_25_q0246  doc_25  Internet Corporation for Assigned Names and Nu...   \n",
      "371  doc_38_q0372  doc_38             Matter, energy, force, space, and time   \n",
      "\n",
      "                                         evidence_span  word_count  \n",
      "180  the object is to get the ball over a goal line...           7  \n",
      "215  The six stika schools of Hindu philosophy that...           6  \n",
      "245  the Domain Name System are directed by a maint...           7  \n",
      "371  hysics is the scientific study of matter its f...           6  \n"
     ]
    }
   ],
   "source": [
    "answers_df[\"word_count\"] = answers_df[\"answer\"].apply(\n",
    "    lambda x: len(x.split()) if x != \"NA\" else 0\n",
    ")\n",
    "\n",
    "print(\"Answers >5 words:\")\n",
    "print(answers_df[answers_df[\"word_count\"] > 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d561f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.drop(columns=[\"word_count\"], errors=\"ignore\").to_csv(\n",
    "    answers_path,\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21a4f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 768.71it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embedder = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(\"MiniLM embedding model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda08fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07e74e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 150\n",
    "OVERLAP = 30\n",
    "\n",
    "def chunk_text(text):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + CHUNK_SIZE\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += CHUNK_SIZE - OVERLAP\n",
    "        \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6539ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: (449, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>doc_1_chunk_0</td>\n",
       "      <td>Albert Einstein was a German-born theoretical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>doc_1_chunk_1</td>\n",
       "      <td>year later which he kept for the rest of his l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>doc_1_chunk_2</td>\n",
       "      <td>he endorsed a letter to President Franklin D. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>doc_1_chunk_3</td>\n",
       "      <td>structure and evolution of the universe as a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>doc_1_chunk_4</td>\n",
       "      <td>physicist Satyendra Nath Bose he laid the grou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id       chunk_id                                         chunk_text\n",
       "0  doc_1  doc_1_chunk_0  Albert Einstein was a German-born theoretical ...\n",
       "1  doc_1  doc_1_chunk_1  year later which he kept for the rest of his l...\n",
       "2  doc_1  doc_1_chunk_2  he endorsed a letter to President Franklin D. ...\n",
       "3  doc_1  doc_1_chunk_3  structure and evolution of the universe as a w...\n",
       "4  doc_1  doc_1_chunk_4  physicist Satyendra Nath Bose he laid the grou..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_records = []\n",
    "\n",
    "for _, row in documents_df.iterrows():\n",
    "    doc_id = row[\"doc_id\"]\n",
    "    text = row[\"document_text\"]\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_records.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"chunk_text\": chunk\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "chunks_df = pd.DataFrame(chunk_records)\n",
    "\n",
    "print(\"Total chunks:\", chunks_df.shape)\n",
    "chunks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5e5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3908dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk embeddings shape: (449, 384)\n"
     ]
    }
   ],
   "source": [
    "chunk_embeddings = embed_texts(\n",
    "    chunks_df[\"chunk_text\"].tolist()\n",
    ")\n",
    "\n",
    "print(\"Chunk embeddings shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "062ee403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built successfully.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = chunk_embeddings.shape[1]  # should be 384\n",
    "index = faiss.IndexFlatIP(dimension)   # cosine similarity (since normalized)\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "print(\"FAISS index built successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6737c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query, k=3):\n",
    "    query_embedding = embed_texts([query])\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    return chunks_df.iloc[indices[0]][\"chunk_text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dadd70ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "physicist Satyendra Nath Bose he laid the groundwork for BoseEinstein statistics. For much of the last phase of his academic life Einstein worked on two endeavors that ultimately proved unsuccessful. First he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world objecting that \"God does not play dice\". Second he attempted to devise a unified \n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "Albert Einstein was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory. His massenergy equivalence formula E = mc2 which arises from special relativity has been called \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics for \"his services to theoretical physics and especiall\n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "Wrttemberg in the German Empire on 14 March 1879. His parents secular Ashkenazi Jews were Hermann Einstein a salesman and engineer and Pauline Koch. In 1880 the family moved to Munich's borough of Ludwigsvorstadt-Isarvorstadt where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein & Cie a company that manufactured electrical equipment based on direct current. When \n"
     ]
    }
   ],
   "source": [
    "test_question = queries_df.iloc[0][\"question\"]\n",
    "retrieved = retrieve_top_k(test_question)\n",
    "\n",
    "for i, chunk in enumerate(retrieved):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n\")\n",
    "    print(chunk[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cc6585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7d902d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Where was Einstein born?\n",
      "Prediction: Ulm in the Kingdom of Wrttemberg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "MODEL_NAME = \"llama3:8b\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You must answer using ONLY the exact phrase from the context.\n",
    "\n",
    "Rules:\n",
    "- Maximum 5 words.\n",
    "- Do NOT rephrase.\n",
    "- Do NOT explain.\n",
    "- Do NOT add extra words.\n",
    "- If the answer is not explicitly written in the context, output: NA\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(question, k=3):\n",
    "    chunks = retrieve_top_k(question, k=k)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        retrieved_chunks=context,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 10\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "# Test one question only\n",
    "row = queries_df.iloc[0]\n",
    "question = row[\"question\"]\n",
    "\n",
    "prompt = build_prompt(question)\n",
    "prediction = generate_with_ollama(prompt)\n",
    "\n",
    "print(\"Q:\", question)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21111b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Where was Einstein born?\n",
      "Prediction: Ulm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1️⃣ Correct Ollama Model Name\n",
    "# --------------------------------------------------\n",
    "MODEL_NAME = \"llama3:8b\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2️⃣ Stronger Extraction Prompt\n",
    "# --------------------------------------------------\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Extract the shortest exact answer from the context.\n",
    "\n",
    "Strict Rules:\n",
    "- Copy the answer exactly from the context.\n",
    "- Use the minimum number of words possible.\n",
    "- Maximum 5 words.\n",
    "- Do NOT include surrounding phrases.\n",
    "- Do NOT explain.\n",
    "- Do NOT add punctuation.\n",
    "- Output only the answer text.\n",
    "- If the answer is not explicitly written in the context, output: NA\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3️⃣ Build RAG Prompt\n",
    "# --------------------------------------------------\n",
    "def build_prompt(question, k=3):\n",
    "    chunks = retrieve_top_k(question, k=k)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        retrieved_chunks=context,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4️⃣ Ollama Generation Function\n",
    "# --------------------------------------------------\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5️⃣ Test One Question\n",
    "# --------------------------------------------------\n",
    "row = queries_df.iloc[0]\n",
    "question = row[\"question\"]\n",
    "\n",
    "prompt = build_prompt(question)\n",
    "prediction = generate_with_ollama(prompt)\n",
    "\n",
    "print(\"Q:\", question)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf9265cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [22:12<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1️⃣ Model Name\n",
    "# --------------------------------------------------\n",
    "MODEL_NAME = \"llama3:8b\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2️⃣ Strict Prompt\n",
    "# --------------------------------------------------\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Extract the shortest exact answer from the context.\n",
    "\n",
    "Strict Rules:\n",
    "- Copy the answer exactly from the context.\n",
    "- Use the minimum number of words possible.\n",
    "- Maximum 5 words.\n",
    "- Do NOT include surrounding phrases.\n",
    "- Do NOT explain.\n",
    "- Do NOT add punctuation.\n",
    "- Output only the answer text.\n",
    "- If the answer is not explicitly written in the context, output: NA\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3️⃣ Build Prompt\n",
    "# --------------------------------------------------\n",
    "def build_prompt(question, k=3):\n",
    "    chunks = retrieve_top_k(question, k=k)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        retrieved_chunks=context,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4️⃣ Ollama Generation\n",
    "# --------------------------------------------------\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5️⃣ Run Over ALL Queries\n",
    "# --------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n",
    "    \n",
    "    query_id = row[\"query_id\"]\n",
    "    doc_id = row[\"doc_id\"]\n",
    "    question = row[\"question\"]\n",
    "    \n",
    "    # Get gold answer\n",
    "    gold = answers_df[\n",
    "        answers_df[\"query_id\"] == query_id\n",
    "    ][\"answer\"].values[0]\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = build_prompt(question)\n",
    "    \n",
    "    # Generate answer\n",
    "    prediction = generate_with_ollama(prompt)\n",
    "    \n",
    "    results.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"question\": question,\n",
    "        \"gold_answer\": gold,\n",
    "        \"model_answer\": prediction\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89d7dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: E:\\Project2\\data\\en\\llama3_results.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = r\"E:\\Project2\\data\\en\\llama3_results.csv\"\n",
    "results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Results saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bc03b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty or NaN gold answers with \"NA\"\n",
    "results_df[\"gold_answer\"] = results_df[\"gold_answer\"].fillna(\"NA\")\n",
    "results_df[\"gold_answer\"] = results_df[\"gold_answer\"].replace(\"\", \"NA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c75854e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"gold_answer\"].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "940ab37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA NO-RAG generation complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1️⃣ Model Name\n",
    "# ----------------------------------------\n",
    "MODEL_NAME = \"llama3:8b\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2️⃣ Strict NO-RAG Prompt\n",
    "# ----------------------------------------\n",
    "NO_RAG_PROMPT = \"\"\"\n",
    "Answer the question in at most 5 words.\n",
    "\n",
    "Strict Rules:\n",
    "- Maximum 5 words.\n",
    "- Do NOT explain.\n",
    "- Do NOT add extra text.\n",
    "- Do NOT add punctuation.\n",
    "- Output only the answer.\n",
    "- If unsure, output: NA\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3️⃣ Ollama Generation Function\n",
    "# ----------------------------------------\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4️⃣ Build NO-RAG Prompt\n",
    "# ----------------------------------------\n",
    "def build_no_rag_prompt(question):\n",
    "    return NO_RAG_PROMPT.format(question=question)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5️⃣ Generate NO-RAG Answers\n",
    "# ----------------------------------------\n",
    "no_rag_predictions = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    \n",
    "    prompt = build_no_rag_prompt(question)\n",
    "    prediction = generate_with_ollama(prompt)\n",
    "    \n",
    "    no_rag_predictions.append(prediction)\n",
    "\n",
    "# Save in NEW column\n",
    "results_df[\"llama_no_rag\"] = no_rag_predictions\n",
    "\n",
    "print(\"LLaMA NO-RAG generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32f94bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "output_path = r\"E:\\Project2\\data\\en\\llama3_results_full.csv\"\n",
    "\n",
    "results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6c38a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"NA\"\n",
    "    return str(text).strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d65a0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"gold_norm\"] = results_df[\"gold_answer\"].apply(normalize_text)\n",
    "results_df[\"rag_norm\"] = results_df[\"model_answer\"].apply(normalize_text)\n",
    "results_df[\"no_rag_norm\"] = results_df[\"llama_no_rag\"].apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "425f0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"rag_exact\"] = (\n",
    "    results_df[\"gold_norm\"] == results_df[\"rag_norm\"]\n",
    ").astype(int)\n",
    "\n",
    "results_df[\"no_rag_exact\"] = (\n",
    "    results_df[\"gold_norm\"] == results_df[\"no_rag_norm\"]\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f1495a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_match(gold, pred):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "results_df[\"rag_substring\"] = results_df.apply(\n",
    "    lambda x: substring_match(x[\"gold_norm\"], x[\"rag_norm\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df[\"no_rag_substring\"] = results_df.apply(\n",
    "    lambda x: substring_match(x[\"gold_norm\"], x[\"no_rag_norm\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f423660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def semantic_score(gold, pred):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6692c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"rag_semantic\"] = results_df.apply(\n",
    "    lambda x: semantic_score(x[\"gold_norm\"], x[\"rag_norm\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df[\"no_rag_semantic\"] = results_df.apply(\n",
    "    lambda x: semantic_score(x[\"gold_norm\"], x[\"no_rag_norm\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df72fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"rag_semantic_match\"] = (\n",
    "    results_df[\"rag_semantic\"] >= 0.50\n",
    ").astype(int)\n",
    "\n",
    "results_df[\"no_rag_semantic_match\"] = (\n",
    "    results_df[\"no_rag_semantic\"] >= 0.50\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1435355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== LLaMA PERFORMANCE =====\n",
      "\n",
      "Exact Match:\n",
      "RAG: 0.624\n",
      "No-RAG: 0.228\n",
      "\n",
      "Substring Match:\n",
      "RAG: 0.536\n",
      "No-RAG: 0.268\n",
      "\n",
      "Semantic Match:\n",
      "RAG: 0.596\n",
      "No-RAG: 0.458\n"
     ]
    }
   ],
   "source": [
    "print(\"===== LLaMA PERFORMANCE =====\")\n",
    "\n",
    "print(\"\\nExact Match:\")\n",
    "print(\"RAG:\", results_df[\"rag_exact\"].mean())\n",
    "print(\"No-RAG:\", results_df[\"no_rag_exact\"].mean())\n",
    "\n",
    "print(\"\\nSubstring Match:\")\n",
    "print(\"RAG:\", results_df[\"rag_substring\"].mean())\n",
    "print(\"No-RAG:\", results_df[\"no_rag_substring\"].mean())\n",
    "\n",
    "print(\"\\nSemantic Match:\")\n",
    "print(\"RAG:\", results_df[\"rag_semantic_match\"].mean())\n",
    "print(\"No-RAG:\", results_df[\"no_rag_semantic_match\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6e0aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Match (Threshold = 0.75):\n",
      "RAG: 0.54\n",
      "No-RAG: 0.348\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSemantic Match (Threshold = 0.75):\")\n",
    "print(\"RAG:\", results_df[\"rag_semantic_match\"].mean())\n",
    "print(\"No-RAG:\", results_df[\"no_rag_semantic_match\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3450d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   gold_norm                              rag_norm  \\\n",
      "0              german empire                                   ulm   \n",
      "1                       1921                                  1921   \n",
      "2                    e = mc2                               e = mc2   \n",
      "3                       1895                                  1895   \n",
      "4          swiss citizenship                                 swiss   \n",
      "5                       1940                                  1940   \n",
      "6   kaiser wilhelm institute  kaiser wilhelm institute for physics   \n",
      "7                         na                                    na   \n",
      "8                         na                                    na   \n",
      "9                         na                                    na   \n",
      "10                      asia                                  asia   \n",
      "11                       30%                                   30%   \n",
      "12               4.7 billion                           4.7 billion   \n",
      "13              indian ocean                                indian   \n",
      "14                suez canal                                  suez   \n",
      "\n",
      "    rag_semantic  \n",
      "0       0.212242  \n",
      "1       1.000000  \n",
      "2       1.000000  \n",
      "3       1.000000  \n",
      "4       0.709989  \n",
      "5       1.000000  \n",
      "6       0.892686  \n",
      "7       0.000000  \n",
      "8       0.000000  \n",
      "9       0.000000  \n",
      "10      1.000000  \n",
      "11      1.000000  \n",
      "12      1.000000  \n",
      "13      0.500488  \n",
      "14      0.898211  \n"
     ]
    }
   ],
   "source": [
    "print(results_df[[\"gold_norm\", \"rag_norm\", \"rag_semantic\"]].head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55096de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    500.000000\n",
      "mean       0.570618\n",
      "std        0.443530\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.788177\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: rag_semantic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(results_df[\"rag_semantic\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cafc6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    500.000000\n",
      "mean       0.570618\n",
      "std        0.443530\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.788177\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: rag_semantic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(results_df[\"rag_semantic\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07771251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SEMANTIC MATCH (Threshold = 0.75) =====\n",
      "RAG Semantic Match: 0.512\n",
      "No-RAG Semantic Match: 0.242\n",
      "\n",
      "Semantic Score Distribution (RAG):\n",
      "count    500.000000\n",
      "mean       0.570618\n",
      "std        0.443530\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.788177\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: rag_semantic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1️⃣ Normalize Text\n",
    "# -----------------------------------------\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "results_df[\"gold_norm\"] = results_df[\"gold_answer\"].apply(normalize_text)\n",
    "results_df[\"rag_norm\"] = results_df[\"model_answer\"].apply(normalize_text)\n",
    "results_df[\"no_rag_norm\"] = results_df[\"llama_no_rag\"].apply(normalize_text)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2️⃣ Semantic Similarity Function\n",
    "# -----------------------------------------\n",
    "def semantic_score(gold, pred):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    \n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    \n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3️⃣ Compute Semantic Scores\n",
    "# -----------------------------------------\n",
    "results_df[\"rag_semantic\"] = results_df.apply(\n",
    "    lambda x: semantic_score(x[\"gold_norm\"], x[\"rag_norm\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df[\"no_rag_semantic\"] = results_df.apply(\n",
    "    lambda x: semantic_score(x[\"gold_norm\"], x[\"no_rag_norm\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4️⃣ Apply Threshold = 0.50\n",
    "# -----------------------------------------\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "results_df[\"rag_semantic_match\"] = (\n",
    "    results_df[\"rag_semantic\"] >= THRESHOLD\n",
    ").astype(int)\n",
    "\n",
    "results_df[\"no_rag_semantic_match\"] = (\n",
    "    results_df[\"no_rag_semantic\"] >= THRESHOLD\n",
    ").astype(int)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5️⃣ Print Final Results\n",
    "# -----------------------------------------\n",
    "print(\"===== SEMANTIC MATCH (Threshold = 0.75) =====\")\n",
    "\n",
    "print(\"RAG Semantic Match:\", results_df[\"rag_semantic_match\"].mean())\n",
    "print(\"No-RAG Semantic Match:\", results_df[\"no_rag_semantic_match\"].mean())\n",
    "\n",
    "print(\"\\nSemantic Score Distribution (RAG):\")\n",
    "print(results_df[\"rag_semantic\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1983e332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>RAG</th>\n",
       "      <th>No-RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Substring Match</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantic Match (0.75)</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Metric    RAG  No-RAG\n",
       "0            Exact Match  0.624   0.228\n",
       "1        Substring Match  0.536   0.268\n",
       "2  Semantic Match (0.75)  0.512   0.242"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Exact Match\",\n",
    "        \"Substring Match\",\n",
    "        \"Semantic Match (0.75)\"\n",
    "    ],\n",
    "    \"RAG\": [\n",
    "        results_df[\"rag_exact\"].mean(),\n",
    "        results_df[\"rag_substring\"].mean(),\n",
    "        results_df[\"rag_semantic_match\"].mean()\n",
    "    ],\n",
    "    \"No-RAG\": [\n",
    "        results_df[\"no_rag_exact\"].mean(),\n",
    "        results_df[\"no_rag_substring\"].mean(),\n",
    "        results_df[\"no_rag_semantic_match\"].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddf1551f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>RAG</th>\n",
       "      <th>No-RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Substring Match</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantic Match (0.75)</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Metric    RAG  No-RAG\n",
       "0            Exact Match  0.624   0.228\n",
       "1        Substring Match  0.536   0.268\n",
       "2  Semantic Match (0.75)  0.512   0.242"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table_rounded = summary_table.copy()\n",
    "summary_table_rounded[\"RAG\"] = summary_table_rounded[\"RAG\"].round(3)\n",
    "summary_table_rounded[\"No-RAG\"] = summary_table_rounded[\"No-RAG\"].round(3)\n",
    "\n",
    "summary_table_rounded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cc0c8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>RAG</th>\n",
       "      <th>No-RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Substring Match</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantic Match (0.75)</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Semantic Score Mean</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantic Score Median</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Semantic Score Std</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semantic Score Min</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Semantic Score Max</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Metric    RAG  No-RAG\n",
       "0            Exact Match  0.624   0.228\n",
       "1        Substring Match  0.536   0.268\n",
       "2  Semantic Match (0.75)  0.512   0.242\n",
       "3    Semantic Score Mean  0.571   0.414\n",
       "4  Semantic Score Median  0.788   0.444\n",
       "5     Semantic Score Std  0.444   0.365\n",
       "6     Semantic Score Min  0.000   0.000\n",
       "7     Semantic Score Max  1.000   1.000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Exact Match\",\n",
    "        \"Substring Match\",\n",
    "        \"Semantic Match (0.75)\",\n",
    "        \"Semantic Score Mean\",\n",
    "        \"Semantic Score Median\",\n",
    "        \"Semantic Score Std\",\n",
    "        \"Semantic Score Min\",\n",
    "        \"Semantic Score Max\"\n",
    "    ],\n",
    "    \"RAG\": [\n",
    "        results_df[\"rag_exact\"].mean(),\n",
    "        results_df[\"rag_substring\"].mean(),\n",
    "        results_df[\"rag_semantic_match\"].mean(),\n",
    "        results_df[\"rag_semantic\"].mean(),\n",
    "        results_df[\"rag_semantic\"].median(),\n",
    "        results_df[\"rag_semantic\"].std(),\n",
    "        results_df[\"rag_semantic\"].min(),\n",
    "        results_df[\"rag_semantic\"].max()\n",
    "    ],\n",
    "    \"No-RAG\": [\n",
    "        results_df[\"no_rag_exact\"].mean(),\n",
    "        results_df[\"no_rag_substring\"].mean(),\n",
    "        results_df[\"no_rag_semantic_match\"].mean(),\n",
    "        results_df[\"no_rag_semantic\"].mean(),\n",
    "        results_df[\"no_rag_semantic\"].median(),\n",
    "        results_df[\"no_rag_semantic\"].std(),\n",
    "        results_df[\"no_rag_semantic\"].min(),\n",
    "        results_df[\"no_rag_semantic\"].max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Round for cleaner display\n",
    "summary_table_rounded = summary_table.copy()\n",
    "summary_table_rounded[\"RAG\"] = summary_table_rounded[\"RAG\"].round(3)\n",
    "summary_table_rounded[\"No-RAG\"] = summary_table_rounded[\"No-RAG\"].round(3)\n",
    "\n",
    "summary_table_rounded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e15eaea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percentile</th>\n",
       "      <th>RAG</th>\n",
       "      <th>No-RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50% (Median)</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Percentile    RAG  No-RAG\n",
       "0           25%  0.000   0.000\n",
       "1  50% (Median)  0.788   0.444\n",
       "2           75%  1.000   0.739"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quartile_table = pd.DataFrame({\n",
    "    \"Percentile\": [\"25%\", \"50% (Median)\", \"75%\"],\n",
    "    \"RAG\": [\n",
    "        results_df[\"rag_semantic\"].quantile(0.25),\n",
    "        results_df[\"rag_semantic\"].quantile(0.50),\n",
    "        results_df[\"rag_semantic\"].quantile(0.75)\n",
    "    ],\n",
    "    \"No-RAG\": [\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.25),\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.50),\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.75)\n",
    "    ]\n",
    "})\n",
    "\n",
    "quartile_table = quartile_table.round(3)\n",
    "\n",
    "quartile_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9cc3f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>RAG</th>\n",
       "      <th>No-RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Substring Match</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantic Match (≥ 0.75)</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Semantic Mean</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantic Std</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Semantic 25%</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semantic 50% (Median)</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Semantic 75%</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Semantic Min</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Semantic Max</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metric    RAG  No-RAG\n",
       "0              Exact Match  0.624   0.228\n",
       "1          Substring Match  0.536   0.268\n",
       "2  Semantic Match (≥ 0.75)  0.512   0.242\n",
       "3            Semantic Mean  0.571   0.414\n",
       "4             Semantic Std  0.444   0.365\n",
       "5             Semantic 25%  0.000   0.000\n",
       "6    Semantic 50% (Median)  0.788   0.444\n",
       "7             Semantic 75%  1.000   0.739\n",
       "8             Semantic Min  0.000   0.000\n",
       "9             Semantic Max  1.000   1.000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "master_table = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Exact Match\",\n",
    "        \"Substring Match\",\n",
    "        f\"Semantic Match (≥ {THRESHOLD})\",\n",
    "        \"Semantic Mean\",\n",
    "        \"Semantic Std\",\n",
    "        \"Semantic 25%\",\n",
    "        \"Semantic 50% (Median)\",\n",
    "        \"Semantic 75%\",\n",
    "        \"Semantic Min\",\n",
    "        \"Semantic Max\"\n",
    "    ],\n",
    "    \"RAG\": [\n",
    "        results_df[\"rag_exact\"].mean(),\n",
    "        results_df[\"rag_substring\"].mean(),\n",
    "        results_df[\"rag_semantic_match\"].mean(),\n",
    "        results_df[\"rag_semantic\"].mean(),\n",
    "        results_df[\"rag_semantic\"].std(),\n",
    "        results_df[\"rag_semantic\"].quantile(0.25),\n",
    "        results_df[\"rag_semantic\"].quantile(0.50),\n",
    "        results_df[\"rag_semantic\"].quantile(0.75),\n",
    "        results_df[\"rag_semantic\"].min(),\n",
    "        results_df[\"rag_semantic\"].max()\n",
    "    ],\n",
    "    \"No-RAG\": [\n",
    "        results_df[\"no_rag_exact\"].mean(),\n",
    "        results_df[\"no_rag_substring\"].mean(),\n",
    "        results_df[\"no_rag_semantic_match\"].mean(),\n",
    "        results_df[\"no_rag_semantic\"].mean(),\n",
    "        results_df[\"no_rag_semantic\"].std(),\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.25),\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.50),\n",
    "        results_df[\"no_rag_semantic\"].quantile(0.75),\n",
    "        results_df[\"no_rag_semantic\"].min(),\n",
    "        results_df[\"no_rag_semantic\"].max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Round for clean display\n",
    "master_table[\"RAG\"] = master_table[\"RAG\"].round(3)\n",
    "master_table[\"No-RAG\"] = master_table[\"No-RAG\"].round(3)\n",
    "\n",
    "master_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2a8b46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [43:06<00:00,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"qwen2.5:7b\"\n",
    "\n",
    "# ----------------------------\n",
    "# RAG Prompt\n",
    "# ----------------------------\n",
    "RAG_PROMPT = \"\"\"\n",
    "Extract the shortest exact answer from the context.\n",
    "\n",
    "Strict Rules:\n",
    "- Copy the answer exactly from the context.\n",
    "- Use the minimum number of words possible.\n",
    "- Maximum 5 words.\n",
    "- Do NOT include surrounding phrases.\n",
    "- Do NOT explain.\n",
    "- Do NOT add punctuation.\n",
    "- Output only the answer text.\n",
    "- If the answer is not explicitly written in the context, output: NA\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# NO-RAG Prompt\n",
    "# ----------------------------\n",
    "NO_RAG_PROMPT = \"\"\"\n",
    "Answer the question in at most 5 words.\n",
    "\n",
    "Strict Rules:\n",
    "- Maximum 5 words.\n",
    "- Do NOT explain.\n",
    "- Do NOT add extra text.\n",
    "- Output only the answer.\n",
    "- If unsure, output: NA\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Ollama Call\n",
    "# ----------------------------\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "# ----------------------------\n",
    "# Run Inference\n",
    "# ----------------------------\n",
    "qwen_rag = []\n",
    "qwen_no_rag = []\n",
    "\n",
    "for _, row in tqdm(results_df.iterrows(), total=len(results_df)):\n",
    "    \n",
    "    question = row[\"question\"]\n",
    "    \n",
    "    # RAG\n",
    "    chunks = retrieve_top_k(question, k=3)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    rag_prompt = RAG_PROMPT.format(\n",
    "        retrieved_chunks=context,\n",
    "        question=question\n",
    "    )\n",
    "    rag_answer = generate_with_ollama(rag_prompt)\n",
    "    qwen_rag.append(rag_answer)\n",
    "    \n",
    "    # NO-RAG\n",
    "    no_rag_prompt = NO_RAG_PROMPT.format(question=question)\n",
    "    no_rag_answer = generate_with_ollama(no_rag_prompt)\n",
    "    qwen_no_rag.append(no_rag_answer)\n",
    "\n",
    "results_df[\"qwen_rag\"] = qwen_rag\n",
    "results_df[\"qwen_no_rag\"] = qwen_no_rag\n",
    "\n",
    "print(\"Qwen inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35c12338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions: 500\n",
      "Answerable Questions: 350\n",
      "Non-Answerable Questions: 150\n",
      "\n",
      "===== ANSWERABLE QUESTIONS =====\n",
      "                  Metric    RAG  No-RAG\n",
      "0            Exact Match  0.540   0.089\n",
      "1        Substring Match  0.771   0.386\n",
      "2  Semantic Match (0.75)  0.731   0.346\n",
      "3    Semantic Mean Score  0.815   0.591\n",
      "\n",
      "===== NON-ANSWERABLE QUESTIONS =====\n",
      "                  Metric   RAG  No-RAG\n",
      "0  Correct NA Prediction  0.82   0.553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==============================\n",
    "# LOAD CSV\n",
    "# ==============================\n",
    "\n",
    "df = pd.read_csv(r\"E:\\Project2\\data\\en\\llama3_results_full.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ==============================\n",
    "# NORMALIZE TEXT\n",
    "# ==============================\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "df[\"gold_norm\"] = df[\"gold_answer\"].apply(normalize_text)\n",
    "df[\"rag_norm\"] = df[\"model_answer\"].apply(normalize_text)\n",
    "df[\"no_rag_norm\"] = df[\"llama_no_rag\"].apply(normalize_text)\n",
    "\n",
    "# ==============================\n",
    "# SPLIT DATA\n",
    "# ==============================\n",
    "\n",
    "answerable_df = df[df[\"gold_norm\"] != \"na\"].copy()\n",
    "non_answerable_df = df[df[\"gold_norm\"] == \"na\"].copy()\n",
    "\n",
    "print(\"Total Questions:\", len(df))\n",
    "print(\"Answerable Questions:\", len(answerable_df))\n",
    "print(\"Non-Answerable Questions:\", len(non_answerable_df))\n",
    "\n",
    "# ==============================\n",
    "# METRIC FUNCTIONS\n",
    "# ==============================\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred == gold)\n",
    "\n",
    "def substring_match(pred, gold):\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "def semantic_score(pred, gold):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    \n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_answerable(data, pred_column, threshold=0.75):\n",
    "    exact_scores = []\n",
    "    substring_scores = []\n",
    "    semantic_scores_list = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        gold = row[\"gold_norm\"]\n",
    "\n",
    "        exact_scores.append(exact_match(pred, gold))\n",
    "        substring_scores.append(substring_match(pred, gold))\n",
    "        semantic_scores_list.append(semantic_score(pred, gold))\n",
    "\n",
    "    semantic_binary = [int(s >= threshold) for s in semantic_scores_list]\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": np.mean(exact_scores),\n",
    "        \"Substring Match\": np.mean(substring_scores),\n",
    "        \"Semantic Match (0.75)\": np.mean(semantic_binary),\n",
    "        \"Semantic Mean Score\": np.mean(semantic_scores_list)\n",
    "    }\n",
    "\n",
    "print(\"\\n===== ANSWERABLE QUESTIONS =====\")\n",
    "\n",
    "rag_answerable = evaluate_answerable(answerable_df, \"rag_norm\")\n",
    "no_rag_answerable = evaluate_answerable(answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_answerable = pd.DataFrame({\n",
    "    \"Metric\": rag_answerable.keys(),\n",
    "    \"RAG\": rag_answerable.values(),\n",
    "    \"No-RAG\": no_rag_answerable.values()\n",
    "}).round(3)\n",
    "\n",
    "print(results_answerable)\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE NON-ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_non_answerable(data, pred_column):\n",
    "    correct_na = []\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        correct_na.append(int(pred == \"na\"))\n",
    "    return np.mean(correct_na)\n",
    "\n",
    "print(\"\\n===== NON-ANSWERABLE QUESTIONS =====\")\n",
    "\n",
    "rag_na_score = evaluate_non_answerable(non_answerable_df, \"rag_norm\")\n",
    "no_rag_na_score = evaluate_non_answerable(non_answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_na = pd.DataFrame({\n",
    "    \"Metric\": [\"Correct NA Prediction\"],\n",
    "    \"RAG\": [rag_na_score],\n",
    "    \"No-RAG\": [no_rag_na_score]\n",
    "}).round(3)\n",
    "\n",
    "print(results_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f768e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Qwen CSV saved successfully.\n",
      "Saved at: E:\\Project2\\data\\en\\qwen_results_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Select only required columns\n",
    "qwen_export_df = results_df[[\n",
    "    \"query_id\",\n",
    "    \"doc_id\",\n",
    "    \"question\",\n",
    "    \"gold_answer\",\n",
    "    \"qwen_rag\",\n",
    "    \"qwen_no_rag\"\n",
    "]].copy()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"E:\\Project2\\data\\en\\qwen_results_full.csv\"\n",
    "\n",
    "qwen_export_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Clean Qwen CSV saved successfully.\")\n",
    "print(\"Saved at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d32f9efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions: 500\n",
      "Answerable Questions: 350\n",
      "Non-Answerable Questions: 150\n",
      "\n",
      "===== ANSWERABLE QUESTIONS (QWEN) =====\n",
      "                  Metric    RAG  No-RAG\n",
      "0            Exact Match  0.609   0.211\n",
      "1        Substring Match  0.774   0.331\n",
      "2  Semantic Match (0.75)  0.786   0.363\n",
      "3    Semantic Mean Score  0.850   0.489\n",
      "\n",
      "===== NON-ANSWERABLE QUESTIONS (QWEN) =====\n",
      "                  Metric    RAG  No-RAG\n",
      "0  Correct NA Prediction  0.873    0.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==============================\n",
    "# LOAD CSV (UTF-8)\n",
    "# ==============================\n",
    "\n",
    "df = pd.read_csv(\n",
    "    r\"E:\\Project2\\data\\en\\qwen_results_full.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# UNICODE NORMALIZATION\n",
    "# ==============================\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalize Unicode (important for Tamil/Malayalam later)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # Strip whitespace and lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    if text == \"\":\n",
    "        return \"na\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "df[\"gold_norm\"] = df[\"gold_answer\"].apply(normalize_text)\n",
    "df[\"rag_norm\"] = df[\"qwen_rag\"].apply(normalize_text)\n",
    "df[\"no_rag_norm\"] = df[\"qwen_no_rag\"].apply(normalize_text)\n",
    "\n",
    "# ==============================\n",
    "# SPLIT DATA\n",
    "# ==============================\n",
    "\n",
    "answerable_df = df[df[\"gold_norm\"] != \"na\"].copy()\n",
    "non_answerable_df = df[df[\"gold_norm\"] == \"na\"].copy()\n",
    "\n",
    "print(\"Total Questions:\", len(df))\n",
    "print(\"Answerable Questions:\", len(answerable_df))\n",
    "print(\"Non-Answerable Questions:\", len(non_answerable_df))\n",
    "\n",
    "# ==============================\n",
    "# METRIC FUNCTIONS\n",
    "# ==============================\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred == gold)\n",
    "\n",
    "def substring_match(pred, gold):\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "def semantic_score(pred, gold):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    \n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_answerable(data, pred_column, threshold=0.75):\n",
    "    exact_scores = []\n",
    "    substring_scores = []\n",
    "    semantic_scores_list = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        gold = row[\"gold_norm\"]\n",
    "\n",
    "        exact_scores.append(exact_match(pred, gold))\n",
    "        substring_scores.append(substring_match(pred, gold))\n",
    "        semantic_scores_list.append(semantic_score(pred, gold))\n",
    "\n",
    "    semantic_binary = [int(s >= threshold) for s in semantic_scores_list]\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": np.mean(exact_scores),\n",
    "        \"Substring Match\": np.mean(substring_scores),\n",
    "        \"Semantic Match (0.75)\": np.mean(semantic_binary),\n",
    "        \"Semantic Mean Score\": np.mean(semantic_scores_list)\n",
    "    }\n",
    "\n",
    "print(\"\\n===== ANSWERABLE QUESTIONS (QWEN) =====\")\n",
    "\n",
    "rag_answerable = evaluate_answerable(answerable_df, \"rag_norm\")\n",
    "no_rag_answerable = evaluate_answerable(answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_answerable = pd.DataFrame({\n",
    "    \"Metric\": rag_answerable.keys(),\n",
    "    \"RAG\": rag_answerable.values(),\n",
    "    \"No-RAG\": no_rag_answerable.values()\n",
    "}).round(3)\n",
    "\n",
    "print(results_answerable)\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE NON-ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_non_answerable(data, pred_column):\n",
    "    correct_na = []\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        correct_na.append(int(pred == \"na\"))\n",
    "    return np.mean(correct_na)\n",
    "\n",
    "print(\"\\n===== NON-ANSWERABLE QUESTIONS (QWEN) =====\")\n",
    "\n",
    "rag_na_score = evaluate_non_answerable(non_answerable_df, \"rag_norm\")\n",
    "no_rag_na_score = evaluate_non_answerable(non_answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_na = pd.DataFrame({\n",
    "    \"Metric\": [\"Correct NA Prediction\"],\n",
    "    \"RAG\": [rag_na_score],\n",
    "    \"No-RAG\": [no_rag_na_score]\n",
    "}).round(3)\n",
    "\n",
    "print(results_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18d0b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [42:58<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral inference complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"mistral:7b\"\n",
    "\n",
    "# ----------------------------\n",
    "# RAG Prompt\n",
    "# ----------------------------\n",
    "RAG_PROMPT = \"\"\"\n",
    "Extract the shortest exact answer from the context.\n",
    "\n",
    "Strict Rules:\n",
    "- Copy the answer exactly from the context.\n",
    "- Use the minimum number of words possible.\n",
    "- Maximum 5 words.\n",
    "- Do NOT include surrounding phrases.\n",
    "- Do NOT explain.\n",
    "- Do NOT add punctuation.\n",
    "- Output only the answer text.\n",
    "- If the answer is not explicitly written in the context, output: NA\n",
    "\n",
    "Context:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# NO-RAG Prompt\n",
    "# ----------------------------\n",
    "NO_RAG_PROMPT = \"\"\"\n",
    "Answer the question in at most 5 words.\n",
    "\n",
    "Strict Rules:\n",
    "- Maximum 5 words.\n",
    "- Do NOT explain.\n",
    "- Do NOT add extra text.\n",
    "- Output only the answer.\n",
    "- If unsure, output: NA\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"num_predict\": 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    return result[\"response\"].strip()\n",
    "\n",
    "mistral_rag = []\n",
    "mistral_no_rag = []\n",
    "\n",
    "for _, row in tqdm(results_df.iterrows(), total=len(results_df)):\n",
    "    \n",
    "    question = row[\"question\"]\n",
    "    \n",
    "    # RAG\n",
    "    chunks = retrieve_top_k(question, k=3)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    rag_prompt = RAG_PROMPT.format(\n",
    "        retrieved_chunks=context,\n",
    "        question=question\n",
    "    )\n",
    "    rag_answer = generate_with_ollama(rag_prompt)\n",
    "    mistral_rag.append(rag_answer)\n",
    "    \n",
    "    # NO-RAG\n",
    "    no_rag_prompt = NO_RAG_PROMPT.format(question=question)\n",
    "    no_rag_answer = generate_with_ollama(no_rag_prompt)\n",
    "    mistral_no_rag.append(no_rag_answer)\n",
    "\n",
    "results_df[\"mistral_rag\"] = mistral_rag\n",
    "results_df[\"mistral_no_rag\"] = mistral_no_rag\n",
    "\n",
    "print(\"Mistral inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7813a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Mistral CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "mistral_export_df = results_df[[\n",
    "    \"query_id\",\n",
    "    \"doc_id\",\n",
    "    \"question\",\n",
    "    \"gold_answer\",\n",
    "    \"mistral_rag\",\n",
    "    \"mistral_no_rag\"\n",
    "]].copy()\n",
    "\n",
    "output_path = r\"E:\\Project2\\data\\en\\mistral_results_full.csv\"\n",
    "\n",
    "mistral_export_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Clean Mistral CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7870876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions: 500\n",
      "Answerable Questions: 350\n",
      "Non-Answerable Questions: 150\n",
      "\n",
      "===== ANSWERABLE QUESTIONS (MISTRAL) =====\n",
      "                  Metric    RAG  No-RAG\n",
      "0            Exact Match  0.506   0.151\n",
      "1        Substring Match  0.700   0.397\n",
      "2  Semantic Match (0.75)  0.720   0.400\n",
      "3    Semantic Mean Score  0.823   0.621\n",
      "\n",
      "===== NON-ANSWERABLE QUESTIONS (MISTRAL) =====\n",
      "                  Metric   RAG  No-RAG\n",
      "0  Correct NA Prediction  0.08     0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==============================\n",
    "# LOAD CSV (UTF-8)\n",
    "# ==============================\n",
    "\n",
    "df = pd.read_csv(\n",
    "    r\"E:\\Project2\\data\\en\\mistral_results_full.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# UNICODE NORMALIZATION\n",
    "# ==============================\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    if text == \"\":\n",
    "        return \"na\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "df[\"gold_norm\"] = df[\"gold_answer\"].apply(normalize_text)\n",
    "df[\"rag_norm\"] = df[\"mistral_rag\"].apply(normalize_text)\n",
    "df[\"no_rag_norm\"] = df[\"mistral_no_rag\"].apply(normalize_text)\n",
    "\n",
    "# ==============================\n",
    "# SPLIT DATA\n",
    "# ==============================\n",
    "\n",
    "answerable_df = df[df[\"gold_norm\"] != \"na\"].copy()\n",
    "non_answerable_df = df[df[\"gold_norm\"] == \"na\"].copy()\n",
    "\n",
    "print(\"Total Questions:\", len(df))\n",
    "print(\"Answerable Questions:\", len(answerable_df))\n",
    "print(\"Non-Answerable Questions:\", len(non_answerable_df))\n",
    "\n",
    "# ==============================\n",
    "# METRIC FUNCTIONS\n",
    "# ==============================\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred == gold)\n",
    "\n",
    "def substring_match(pred, gold):\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "def semantic_score(pred, gold):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    \n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_answerable(data, pred_column, threshold=0.75):\n",
    "    exact_scores = []\n",
    "    substring_scores = []\n",
    "    semantic_scores_list = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        gold = row[\"gold_norm\"]\n",
    "\n",
    "        exact_scores.append(exact_match(pred, gold))\n",
    "        substring_scores.append(substring_match(pred, gold))\n",
    "        semantic_scores_list.append(semantic_score(pred, gold))\n",
    "\n",
    "    semantic_binary = [int(s >= threshold) for s in semantic_scores_list]\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": np.mean(exact_scores),\n",
    "        \"Substring Match\": np.mean(substring_scores),\n",
    "        \"Semantic Match (0.75)\": np.mean(semantic_binary),\n",
    "        \"Semantic Mean Score\": np.mean(semantic_scores_list)\n",
    "    }\n",
    "\n",
    "print(\"\\n===== ANSWERABLE QUESTIONS (MISTRAL) =====\")\n",
    "\n",
    "rag_answerable = evaluate_answerable(answerable_df, \"rag_norm\")\n",
    "no_rag_answerable = evaluate_answerable(answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_answerable = pd.DataFrame({\n",
    "    \"Metric\": rag_answerable.keys(),\n",
    "    \"RAG\": rag_answerable.values(),\n",
    "    \"No-RAG\": no_rag_answerable.values()\n",
    "}).round(3)\n",
    "\n",
    "print(results_answerable)\n",
    "\n",
    "# ==============================\n",
    "# EVALUATE NON-ANSWERABLE\n",
    "# ==============================\n",
    "\n",
    "def evaluate_non_answerable(data, pred_column):\n",
    "    correct_na = []\n",
    "    for _, row in data.iterrows():\n",
    "        pred = row[pred_column]\n",
    "        correct_na.append(int(pred == \"na\"))\n",
    "    return np.mean(correct_na)\n",
    "\n",
    "print(\"\\n===== NON-ANSWERABLE QUESTIONS (MISTRAL) =====\")\n",
    "\n",
    "rag_na_score = evaluate_non_answerable(non_answerable_df, \"rag_norm\")\n",
    "no_rag_na_score = evaluate_non_answerable(non_answerable_df, \"no_rag_norm\")\n",
    "\n",
    "results_na = pd.DataFrame({\n",
    "    \"Metric\": [\"Correct NA Prediction\"],\n",
    "    \"RAG\": [rag_na_score],\n",
    "    \"No-RAG\": [no_rag_na_score]\n",
    "}).round(3)\n",
    "\n",
    "print(results_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "155bbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "results_df[\"gold_norm\"] = results_df[\"gold_answer\"].apply(normalize_text)\n",
    "\n",
    "results_df[\"rag_norm\"] = results_df[\"model_answer\"].apply(normalize_text)\n",
    "results_df[\"no_rag_norm\"] = results_df[\"llama_no_rag\"].apply(normalize_text)\n",
    "\n",
    "results_df[\"qwen_rag_norm\"] = results_df[\"qwen_rag\"].apply(normalize_text)\n",
    "results_df[\"qwen_no_rag_norm\"] = results_df[\"qwen_no_rag\"].apply(normalize_text)\n",
    "\n",
    "results_df[\"mistral_rag_norm\"] = results_df[\"mistral_rag\"].apply(normalize_text)\n",
    "results_df[\"mistral_no_rag_norm\"] = results_df[\"mistral_no_rag\"].apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6bdae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred == gold)\n",
    "\n",
    "def substring_match(pred, gold):\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "def semantic_score(pred, gold):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "THRESHOLD = 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1f00635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(prefix):\n",
    "    \n",
    "    # Exact\n",
    "    results_df[f\"{prefix}_exact\"] = results_df.apply(\n",
    "        lambda x: exact_match(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Substring\n",
    "    results_df[f\"{prefix}_substring\"] = results_df.apply(\n",
    "        lambda x: substring_match(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Semantic score\n",
    "    results_df[f\"{prefix}_semantic\"] = results_df.apply(\n",
    "        lambda x: semantic_score(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Semantic binary\n",
    "    results_df[f\"{prefix}_semantic_match\"] = (\n",
    "        results_df[f\"{prefix}_semantic\"] >= THRESHOLD\n",
    "    ).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4dfaaae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All evaluation columns created successfully.\n"
     ]
    }
   ],
   "source": [
    "# LLaMA\n",
    "compute_all_metrics(\"rag\")\n",
    "compute_all_metrics(\"no_rag\")\n",
    "\n",
    "# Qwen\n",
    "compute_all_metrics(\"qwen_rag\")\n",
    "compute_all_metrics(\"qwen_no_rag\")\n",
    "\n",
    "# Mistral\n",
    "compute_all_metrics(\"mistral_rag\")\n",
    "compute_all_metrics(\"mistral_no_rag\")\n",
    "\n",
    "print(\"All evaluation columns created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1e1428e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>LLaMA</th>\n",
       "      <th>Qwen</th>\n",
       "      <th>Mistral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match (RAG)</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exact Match (No-RAG)</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Substring Match (RAG)</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Substring Match (No-RAG)</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantic Match ≥ 0.75 (RAG)</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Semantic Match ≥ 0.75 (No-RAG)</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Semantic Mean (RAG)</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Semantic Mean (No-RAG)</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Correct NA (RAG)</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Correct NA (No-RAG)</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Metric  LLaMA   Qwen  Mistral\n",
       "0               Exact Match (RAG)  0.540  0.609    0.506\n",
       "1            Exact Match (No-RAG)  0.089  0.211    0.151\n",
       "2           Substring Match (RAG)  0.771  0.774    0.700\n",
       "3        Substring Match (No-RAG)  0.386  0.331    0.397\n",
       "4     Semantic Match ≥ 0.75 (RAG)  0.731  0.786    0.720\n",
       "5  Semantic Match ≥ 0.75 (No-RAG)  0.346  0.363    0.400\n",
       "6             Semantic Mean (RAG)  0.815  0.850    0.823\n",
       "7          Semantic Mean (No-RAG)  0.591  0.489    0.621\n",
       "8                Correct NA (RAG)  0.820  0.873    0.080\n",
       "9             Correct NA (No-RAG)  0.553  0.940    0.000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "# Split once\n",
    "answerable_df = results_df[results_df[\"gold_norm\"] != \"na\"]\n",
    "non_answerable_df = results_df[results_df[\"gold_norm\"] == \"na\"]\n",
    "\n",
    "# Helper function\n",
    "def compute_metrics(data, rag_exact, rag_sub, rag_sem_match, rag_sem,\n",
    "                    no_rag_exact, no_rag_sub, no_rag_sem_match, no_rag_sem):\n",
    "\n",
    "    return {\n",
    "        \"Exact Match (RAG)\": data[rag_exact].mean(),\n",
    "        \"Exact Match (No-RAG)\": data[no_rag_exact].mean(),\n",
    "\n",
    "        \"Substring Match (RAG)\": data[rag_sub].mean(),\n",
    "        \"Substring Match (No-RAG)\": data[no_rag_sub].mean(),\n",
    "\n",
    "        f\"Semantic Match ≥ {THRESHOLD} (RAG)\": data[rag_sem_match].mean(),\n",
    "        f\"Semantic Match ≥ {THRESHOLD} (No-RAG)\": data[no_rag_sem_match].mean(),\n",
    "\n",
    "        \"Semantic Mean (RAG)\": data[rag_sem].mean(),\n",
    "        \"Semantic Mean (No-RAG)\": data[no_rag_sem].mean(),\n",
    "    }\n",
    "\n",
    "def compute_na(data, rag_col, no_rag_col):\n",
    "    return {\n",
    "        \"Correct NA (RAG)\": (data[rag_col] == \"na\").mean(),\n",
    "        \"Correct NA (No-RAG)\": (data[no_rag_col] == \"na\").mean()\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# LLaMA\n",
    "# -----------------------------\n",
    "llama_answerable = compute_metrics(\n",
    "    answerable_df,\n",
    "    \"rag_exact\", \"rag_substring\", \"rag_semantic_match\", \"rag_semantic\",\n",
    "    \"no_rag_exact\", \"no_rag_substring\", \"no_rag_semantic_match\", \"no_rag_semantic\"\n",
    ")\n",
    "\n",
    "llama_na = compute_na(\n",
    "    non_answerable_df,\n",
    "    \"rag_norm\",\n",
    "    \"no_rag_norm\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# QWEN\n",
    "# -----------------------------\n",
    "qwen_answerable = compute_metrics(\n",
    "    answerable_df,\n",
    "    \"qwen_rag_exact\", \"qwen_rag_substring\", \"qwen_rag_semantic_match\", \"qwen_rag_semantic\",\n",
    "    \"qwen_no_rag_exact\", \"qwen_no_rag_substring\", \"qwen_no_rag_semantic_match\", \"qwen_no_rag_semantic\"\n",
    ")\n",
    "\n",
    "qwen_na = compute_na(\n",
    "    non_answerable_df,\n",
    "    \"qwen_rag_norm\",\n",
    "    \"qwen_no_rag_norm\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# MISTRAL\n",
    "# -----------------------------\n",
    "mistral_answerable = compute_metrics(\n",
    "    answerable_df,\n",
    "    \"mistral_rag_exact\", \"mistral_rag_substring\", \"mistral_rag_semantic_match\", \"mistral_rag_semantic\",\n",
    "    \"mistral_no_rag_exact\", \"mistral_no_rag_substring\", \"mistral_no_rag_semantic_match\", \"mistral_no_rag_semantic\"\n",
    ")\n",
    "\n",
    "mistral_na = compute_na(\n",
    "    non_answerable_df,\n",
    "    \"mistral_rag_norm\",\n",
    "    \"mistral_no_rag_norm\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Combine Everything\n",
    "# -----------------------------\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Metric\": list(llama_answerable.keys()) + list(llama_na.keys()),\n",
    "\n",
    "    \"LLaMA\": list(llama_answerable.values()) + list(llama_na.values()),\n",
    "    \"Qwen\": list(qwen_answerable.values()) + list(qwen_na.values()),\n",
    "    \"Mistral\": list(mistral_answerable.values()) + list(mistral_na.values())\n",
    "})\n",
    "\n",
    "comparison_table = comparison_table.round(3)\n",
    "\n",
    "comparison_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66fa0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics computed.\n",
      "MASTER CSV saved successfully.\n",
      "Saved at: E:\\Project2\\data\\en\\MASTER_EVALUATION_ALL_MODELS.csv\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# -----------------------------------\n",
    "# Ensure normalization exists\n",
    "# -----------------------------------\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"na\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.strip().lower()\n",
    "    if text == \"\":\n",
    "        return \"na\"\n",
    "    return text\n",
    "\n",
    "results_df[\"gold_norm\"] = results_df[\"gold_answer\"].apply(normalize_text)\n",
    "\n",
    "# -----------------------------------\n",
    "# Function to compute metrics\n",
    "# -----------------------------------\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return int(pred == gold)\n",
    "\n",
    "def substring_match(pred, gold):\n",
    "    return int(gold in pred or pred in gold)\n",
    "\n",
    "def semantic_score(pred, gold):\n",
    "    if gold == \"na\" or pred == \"na\":\n",
    "        return 0.0\n",
    "    g_vec = embed_texts([gold])\n",
    "    p_vec = embed_texts([pred])\n",
    "    return cosine_similarity(g_vec, p_vec)[0][0]\n",
    "\n",
    "def compute_metrics(prefix):\n",
    "    results_df[f\"{prefix}_norm\"] = results_df[prefix].apply(normalize_text)\n",
    "    \n",
    "    results_df[f\"{prefix}_exact\"] = results_df.apply(\n",
    "        lambda x: exact_match(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    results_df[f\"{prefix}_substring\"] = results_df.apply(\n",
    "        lambda x: substring_match(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    results_df[f\"{prefix}_semantic\"] = results_df.apply(\n",
    "        lambda x: semantic_score(x[f\"{prefix}_norm\"], x[\"gold_norm\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    results_df[f\"{prefix}_semantic_match\"] = (\n",
    "        results_df[f\"{prefix}_semantic\"] >= THRESHOLD\n",
    "    ).astype(int)\n",
    "\n",
    "# -----------------------------------\n",
    "# Compute for ALL models\n",
    "# -----------------------------------\n",
    "\n",
    "compute_metrics(\"model_answer\")       # LLaMA RAG\n",
    "compute_metrics(\"llama_no_rag\")\n",
    "\n",
    "compute_metrics(\"qwen_rag\")\n",
    "compute_metrics(\"qwen_no_rag\")\n",
    "\n",
    "compute_metrics(\"mistral_rag\")\n",
    "compute_metrics(\"mistral_no_rag\")\n",
    "\n",
    "print(\"All metrics computed.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Create MASTER EXPORT DF\n",
    "# -----------------------------------\n",
    "\n",
    "master_export_df = results_df[[\n",
    "    \"query_id\",\n",
    "    \"doc_id\",\n",
    "    \"question\",\n",
    "    \"gold_answer\",\n",
    "\n",
    "    # LLaMA\n",
    "    \"model_answer\",\n",
    "    \"model_answer_exact\",\n",
    "    \"model_answer_substring\",\n",
    "    \"model_answer_semantic\",\n",
    "    \"model_answer_semantic_match\",\n",
    "\n",
    "    \"llama_no_rag\",\n",
    "    \"llama_no_rag_exact\",\n",
    "    \"llama_no_rag_substring\",\n",
    "    \"llama_no_rag_semantic\",\n",
    "    \"llama_no_rag_semantic_match\",\n",
    "\n",
    "    # Qwen\n",
    "    \"qwen_rag\",\n",
    "    \"qwen_rag_exact\",\n",
    "    \"qwen_rag_substring\",\n",
    "    \"qwen_rag_semantic\",\n",
    "    \"qwen_rag_semantic_match\",\n",
    "\n",
    "    \"qwen_no_rag\",\n",
    "    \"qwen_no_rag_exact\",\n",
    "    \"qwen_no_rag_substring\",\n",
    "    \"qwen_no_rag_semantic\",\n",
    "    \"qwen_no_rag_semantic_match\",\n",
    "\n",
    "    # Mistral\n",
    "    \"mistral_rag\",\n",
    "    \"mistral_rag_exact\",\n",
    "    \"mistral_rag_substring\",\n",
    "    \"mistral_rag_semantic\",\n",
    "    \"mistral_rag_semantic_match\",\n",
    "\n",
    "    \"mistral_no_rag\",\n",
    "    \"mistral_no_rag_exact\",\n",
    "    \"mistral_no_rag_substring\",\n",
    "    \"mistral_no_rag_semantic\",\n",
    "    \"mistral_no_rag_semantic_match\"\n",
    "]].copy()\n",
    "\n",
    "# -----------------------------------\n",
    "# Save MASTER CSV\n",
    "# -----------------------------------\n",
    "\n",
    "output_path = r\"E:\\Project2\\data\\en\\MASTER_EVALUATION_ALL_MODELS.csv\"\n",
    "\n",
    "master_export_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"MASTER CSV saved successfully.\")\n",
    "print(\"Saved at:\", output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
